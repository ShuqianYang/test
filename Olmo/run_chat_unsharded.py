import torch
import pathlib
import numpy as np
from transformers import AutoTokenizer
from omegaconf import OmegaConf

from hf_olmo.modeling_olmo import OLMoForCausalLM
from hf_olmo.configuration_olmo import OLMoConfig

# é¿å… torch.load å‡ºé”™
torch.serialization.add_safe_globals([
    pathlib.PosixPath,
    np.core.multiarray._reconstruct,
    np.ndarray,
    np.dtype,
    np.dtypes.UInt32DType,
])

def load_model_and_ckpt_unsharded():
    # 1. åŠ è½½ Qwen tokenizer
    tokenizer_path = "Qwen/Qwen2.5-0.5B"
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

    # 2. è¯»å– YAML é…ç½®
    yaml_path = "/mnt/zzb/workspace/three_data/train_codes/gj/OLMo/configs/official-0425/OLMo2-1B-stage1.yaml"
    cfg = OmegaConf.load(yaml_path)

    # 3. è®¡ç®— mlp_hidden_size å¹¶æ„é€  config
    cfg.model["mlp_hidden_size"] = cfg.model.d_model * cfg.model.mlp_ratio
    model_config_dict = OmegaConf.to_container(cfg.model, resolve=True)
    config = OLMoConfig.from_dict(model_config_dict)

    # 4. åˆå§‹åŒ–æ¨¡å‹ç»“æ„
    model = OLMoForCausalLM(config)

    # 5. ç›´æ¥åŠ è½½ unsharded checkpoint æ–‡ä»¶ï¼ˆä¿®æ”¹è·¯å¾„ä¸ºä½ çš„å®é™…è·¯å¾„ï¼‰
    ckpt_path = "/mnt/zzb/workspace/three_data/train_codes/gj/OLMo/tmp/Checkpoints/Qwen/OLMo2-1B-stage1/step7400-unsharded/model.pt"
    state_dict = torch.load(ckpt_path, map_location="cpu")
    if "model" in state_dict:
        state_dict = state_dict["model"]

    # 6. å¦‚æœç¼ºå¤± model. å‰ç¼€ï¼ŒåŠ ä¸Š
    if not any(k.startswith("model.") for k in state_dict.keys()):
        state_dict = {"model." + k: v for k, v in state_dict.items()}

    # 7. åŠ è½½æƒé‡åˆ°æ¨¡å‹
    model.load_state_dict(state_dict, strict=False)
    model.eval()
    model.to("cuda")

    return model, tokenizer

def generate_reply(model, tokenizer, prompt, max_new_tokens=50):
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
    if "token_type_ids" in inputs:
        del inputs["token_type_ids"]
    inputs = {k: v.to("cuda") for k, v in inputs.items()}

    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            do_sample=True,
            temperature=0.5,
            top_p=0.9,
            repetition_penalty=1.1,
            max_new_tokens=max_new_tokens,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
            no_repeat_ngram_size=2
        )

    full_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    # å»é™¤é‡å¤ prompt
    if full_text.startswith(prompt):
        full_text = full_text[len(prompt):].strip()

    # å¦‚æœç¬¬ä¸€ä¸ªå­—ç¬¦æ˜¯æ ‡ç‚¹ï¼Œå»æ‰å®ƒ
    if full_text and full_text[0] in ['ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', ',', '.', '!', '?']:
        full_text = full_text[1:].strip()

    # æˆªæ–­åˆ°ç¬¬ä¸€ä¸ªç»ˆæ­¢æ ‡ç‚¹
    END_TOKENS = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']
    for token in END_TOKENS:
        if token in full_text:
            idx = full_text.index(token)
            return full_text[:idx+1].strip()

    return full_text[:max_new_tokens].strip()





def interactive_loop(model, tokenizer):
    print("ğŸ” è¿›å…¥ OLMo å¯¹è¯æ¨¡å¼ï¼Œè¾“å…¥ q é€€å‡ºã€‚")
    while True:
        user_input = input("\nğŸ§‘â€ğŸ’» ä½ ï¼š")
        if user_input.lower().strip() in {"q", "quit", "exit"}:
            print("ğŸ‘‹ å†è§ï¼")
            break
        reply = generate_reply(model, tokenizer, user_input)
        print(f"\nğŸ¤– OLMoï¼š{reply}")

if __name__ == "__main__":
    print("ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹å’Œ tokenizerï¼Œè¯·ç¨å€™...")
    model, tokenizer = load_model_and_ckpt_unsharded()
    interactive_loop(model, tokenizer)
